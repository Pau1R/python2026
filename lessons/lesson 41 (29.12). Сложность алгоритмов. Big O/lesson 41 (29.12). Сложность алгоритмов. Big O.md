# Урок 41. Сложность алгоритмов и Big O

Короткое базовое описание: Big O описывает, как меняется время или память алгоритма при росте входа `n`. Мы не меряем абсолютные секунды, а только темп роста. Это нужно, чтобы заранее видеть узкие места и выбирать структуру данных осознанно.

## Что такое Big O

1. Big O показывает верхнюю границу роста. Фраза «алгоритм `O(n)`» значит, что при увеличении `n` время растет пропорционально размеру входа.
2. Нас интересуют крупные различия: `O(1)` почти не зависит от `n`, `O(n)` растет линейно, `O(n^2)` взрывается при больших объемах, `O(log n)` растет медленно.
3. Мы обычно описываем худший случай. Средний и амортизированный случаи полезны, но всегда уточняй, какую оценку даешь.

## На что обращать внимание

- **Размер входа**: чем больше данные, тем заметнее разница между классами сложности.
- **Операции над структурами**:
  - `list[i]` и `append` в конец дают `O(1)` (последний — амортизированно).
  - Поиск `x in list`, вставка в середину или начало — `O(n)` из-за сдвига или просмотра всех элементов.
  - Операции `dict` и `set` по ключу обычно `O(1)` за счет хеш-таблицы, но требуют дополнительной памяти и деградируют до `O(n)` при большом числе коллизий.
- **Вложенные циклы**: два вложенных прохода по одним и тем же данным чаще всего дают `O(n^2)`. Это сигнал искать более подходящую структуру хранения.

## Базовые классы сложности

| Класс | Интуиция | Типичные примеры |
| --- | --- | --- |
| `O(1)` | Время почти не зависит от входа | чтение `dict[key]`, `len(list)` |
| `O(log n)` | Добавляемый объем данных дает несколько доп. шагов | двоичный поиск, работа с сбалансированными деревьями |
| `O(n)` | Один полный проход | цикл по списку без вложений |
| `O(n log n)` | Линейный проход плюс логарифмическая вложенность | сортировки, которые делят данные пополам |
| `O(n^2)` | Полный проход внутри полного прохода | попарные сравнения, наивные проверки дублей |

## Типичные детали и ловушки

1. **Стоимость подготовки**: построение `set` или `dict` стоит `O(n)`. Это окупается только если после подготовки много запросов.
2. **Амортизированная сложность**: операции типа `list.append` иногда пересоздают буфер и стоят дороже, но средняя цена остается `O(1)` при длинной серии вызовов.
3. **Память против скорости**: `dict`, `set` и кэширование ускоряют поиск, но увеличивают потребление памяти. Следи за балансом.
4. **Худший случай**: даже константные операции могут падать до линейных, если хеш-функция плохая или ключи специально подобраны.

## Мини-примеры

```python
# Линейный проход
def has_value(items, target):
    for value in items:          # O(n)
        if value == target:
            return True
    return False

# Проверка через множество
def has_value_fast(items, target):
    lookup = set(items)          # O(n) подготовка
    return target in lookup      # O(1) запрос
```

```python
# Квадратичный алгоритм: вложенный поиск
def deduplicate_slow(items):
    result = []
    for item in items:           # O(n)
        if item not in result:   # O(n)
            result.append(item)
    return result                # O(n^2)

# Линейный вариант
def deduplicate_fast(items):
    seen = set()
    result = []
    for item in items:           # O(n)
        if item not in seen:     # O(1)
            seen.add(item)
            result.append(item)
    return result                # O(n)
```

## Что запомнить

1. Сложность — это модель роста, а не секундомер. Важен тренд при больших `n`.
2. Следи за операциями, которые скрыто делают линейный обход внутри цикла. Именно они превращают код в `O(n^2)`.
3. `dict` и `set` дают быстрый поиск ценой памяти и необходимости иметь хешируемые ключи.